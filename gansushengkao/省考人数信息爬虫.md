@[TOC](省考人数信息爬虫)
前几周打算报名某省考，然后发现查询报名人数信息十分麻烦，都已经信息化时代十几年了，互联网发展到今天，这些政府官网仍然技术跟不上，所以只能直接获取数据了。
## 网页结构分析
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/9c1846eea90c4612a482c17cb4e5a78c.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/8fc0977a59f34f26b2b4678a2500ad8f.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/64e16ac59d92464db16c1045db9286f7.png)

下面分析一下这个搞笑的网页结构，这个结构没有几十年脑梗写不出这样的结构。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/0f6e38037cf04284acaae0961f21539c.png)
扫码获取技术支持都来了，你复制就好好复制，就看都不看就粘贴过来了是吧。
下面这个图更是搞笑，一个页面里面套两个iframe，左边导航栏是个iframe，右边的页面也是iframe，甚至iframe下面document直接就ip和接口裸奔了，到底是官方网站，丝毫不怕被攻击，哈哈哈，牛！
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/2e859a9519ca474382f8c042600ed827.png)
你以为这就完了，No，单页面iframe的叠加还没完，看下面。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/3b86bb61dc274b69ac46ca5dc91c1d20.png)
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/e24bc96d90bf4698829e5af7e4c4db9d.png)
一个页面，三个iframe，绝！到底是官方网站，都不用测试，这个一个网页系统写成这样，我只能说自动化测试软件Selenium看了都得绕道走。
## 爬虫接口分析
接口还需要分析啥啊，都和IP裸奔了，直接看图说话吧。
http://60.164.220.222:9620/tip/stdrg.do?method=fwdStdPositionStatisQuery&pcid=62924020110000000164
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/cd7ebac6be6f4c9bb9b8e6868e425cd5.png)
点击查询，然后调试网络部分查看查询请求。
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/db740c828c1742ac9a516faf80fbaeeb.png)
然后继续使用编码转换工具（https://sbox.cn/tool/curlconverter），将请求转换为Python编码。

```python
import requests

cookies = {
    'JSESSIONID': '04553CE5017FEB9386D0C7C733AF0881',
    'SF_cookie_26': '18174751',
    'Hm_lvt_ac5110c8893f1f2e7a1be0acf395c2d5': '1710840798',
    'Hm_lpvt_ac5110c8893f1f2e7a1be0acf395c2d5': '1710855014',
}

headers = {
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,de-DE;q=0.5,de;q=0.4',
    'Connection': 'keep-alive',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    # 'Cookie': 'JSESSIONID=04553CE5017FEB9386D0C7C733AF0881; SF_cookie_26=18174751; Hm_lvt_ac5110c8893f1f2e7a1be0acf395c2d5=1710840798; Hm_lpvt_ac5110c8893f1f2e7a1be0acf395c2d5=1710855014',
    'Origin': 'http://60.164.220.222:9620',
    'Referer': 'http://60.164.220.222:9620/tip/stdrg.do?method=fwdStdPositionStatisQuery&pcid=62924020110000000164',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0',
    'X-Requested-With': 'XMLHttpRequest',
}

params = {
    'method': 'queryPositionStatisticsData',
}

data = {
    'zwdm': '0091',
    'pcid': '62924020110000000164',
    'page': '1',
}

response = requests.post(
    'http://60.164.220.222:9620/tip/stdrg.do',
    params=params,
    cookies=cookies,
    headers=headers,
    data=data,
    verify=False,
)
```
分析数据部分，即data字典，可以发现主要是通过发送职位代码和pc的ID以及页码数进行post请求，同时，由于官方防火墙等安全设置等级不高（这样挺好，方便我们获取数据），因此我们直接通过按照职位代码依次请求获取数据即可。
## 爬虫编码实现
主要按照三部分实现：初始化zwdm列表、发送请求并处理响应，获取数据进行存储。

```python
import requests
from openpyxl import Workbook
from tqdm import tqdm

cookies = {
    'JSESSIONID': '04553CE5017FEB9386D0C7C733AF0881',
    'SF_cookie_26': '18174751',
    'Hm_lvt_ac5110c8893f1f2e7a1be0acf395c2d5': '1710840798',
    'Hm_lpvt_ac5110c8893f1f2e7a1be0acf395c2d5': '1710855014',
}

headers = {
    'Accept': 'application/json, text/javascript, */*; q=0.01',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6,de-DE;q=0.5,de;q=0.4',
    'Connection': 'keep-alive',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    # 'Cookie': 'JSESSIONID=04553CE5017FEB9386D0C7C733AF0881; SF_cookie_26=18174751; Hm_lvt_ac5110c8893f1f2e7a1be0acf395c2d5=1710840798; Hm_lpvt_ac5110c8893f1f2e7a1be0acf395c2d5=1710855014',
    'Origin': 'http://60.164.220.222:9620',
    'Referer': 'http://60.164.220.222:9620/tip/stdrg.do?method=fwdStdPositionStatisQuery&pcid=62924020110000000164',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36 Edg/123.0.0.0',
    'X-Requested-With': 'XMLHttpRequest',
}

params = {
    'method': 'queryPositionStatisticsData',
}

data = {
    'zwdm': '0091',
    'pcid': '62924020110000000164',
    'page': '1',
}

response = requests.post(
    'http://60.164.220.222:9620/tip/stdrg.do',
    params=params,
    cookies=cookies,
    headers=headers,
    data=data,
    verify=False,
)
# 初始的zwdm列表，你可以根据需要添加更多的值
# zwdm_list = ['0091', '0001', '0002']
zwdm_list = [str(i).zfill(4) for i in range(1, 4127)]


# 定义一个函数来发送请求并处理响应
def send_request(zwdm):
    data = {
        'zwdm': zwdm,
        'pcid': '62924020110000000164',
        'page': '1',
    }
    response = requests.post(
        'http://60.164.220.222:9620/tip/stdrg.do',
        params=params,
        cookies=cookies,
        headers=headers,
        data=data,
        verify=False,
    )
    return response.json()


if __name__ == '__main__':
    response_data_list=[]
    # 主循环，遍历zwdm列表并发送请求
    # for zwdm in zwdm_list:
    for zwdm in tqdm(zwdm_list, desc="Processing zwdm_list", unit="request"):
        response_data = send_request(zwdm)
        response_data_list.append(response_data)
        # 处理响应数据，例如打印或保存到文件
        # print(response_data)
    # Initialize an empty list to store all results
    all_results = []

    for response_data in response_data_list:
        # Extracting the required information from response_data
        results = []
        for record in response_data.get('dstj', []):
            row = [record.get('dwmc', ''), record.get('zwdm', ''), record.get('zwmc', ''),
                   record.get('jhzkrs', ''), record.get('shtgrs', ''), record.get('jfcgrs', ''),
                   record.get('bmcgrs', '')]
            results.append(row)

        # Append results to all_results
        all_results.extend(results)

            # 将结果写入Excel文件
    wb = Workbook()  # 创建一个新的工作簿
    ws = wb.active  # 获取活动工作表

    # 写入表头
    headers = ['招考单位', '职位代码', '职位名称', '计划招考人数', '审核通过人数', '缴费成功人数', '报名成功人数']
    ws.append(headers)  # 将表头添加到工作表

    # 写入查询结果
    for row in all_results:
        ws.append(row)  # 将每行数据添加到工作表

    # 保存工作簿为Excel文件
    file_name = 'query_results1.xlsx'
    wb.save(file_name)

    print(f"查询结果已保存到文件 {file_name} 中。")

```
其中，我们还通过使用tqdm库进行了请求处理可视化，大概每秒能够处理9-10个请求。在构造职位代码列表时，我们采用Python中的列表推导式（list comprehension），它用于生成一个新的列表zwdm_list。
range(1, 4127): 这是一个Python内置函数，用于生成一个从1开始到4126结束的整数序列（注意，range的上界是不包含的）。
str(i): 将range生成的每个整数i转换为字符串。
.zfill(4): 这是字符串的一个方法，用于在字符串的左侧填充零，直到它达到指定的长度。在这里，每个整数i被转换为字符串后，都会在左侧填充零，直到其长度为4个字符。
[...] for i in range(1, 4127): 这是列表推导式的核心部分。它会遍历range(1, 4127)中的每个整数i，并应用上述的转换操作。
zwdm_list将会是一个包含4126个字符串的列表，这些字符串是从"0001"到"4126"，每个字符串都是四位数，前面用零填充（如果需要的话）。
这样操作是因为职位代码在官方编码过程中即采用了四位数字的固定顺序组合形式，同时，官方在查询的后端请求处理编码逻辑部分并未对缺0或者模糊查询进行处理（体制内，懂的都懂，得过且过），因此不能忽略0的存在，否则缺0占位直接请求1无法获取正确响应输出。
## 结果展示
![在这里插入图片描述](https://img-blog.csdnimg.cn/direct/b3165e5573e44859b4bd24ec5e203d85.png)

